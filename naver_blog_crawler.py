import json
import time
from selenium.common.exceptions import NoSuchElementException
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

from llm_utils import chunk_review_with_llm   # <â€” your LLM chunking function

# Selenium WebDriver setup
options = webdriver.ChromeOptions()
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option("useAutomationExtension", False)

options.binary_location = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
options.add_argument("--headless") # Run in headless mode
service = Service(ChromeDriverManager().install())

driver = webdriver.Chrome(service=service, options=options)
driver.implicitly_wait(3)

# DEBUG: Test Links
links = [
    "https://blog.naver.com/melon_815/222689879387",
    # "https://blog.naver.com/pamada_salon/223584940093",
    # "https://blog.naver.com/ton100ya/222906332890",
    # "https://vikanmichael.tistory.com/2591",
    # "https://j3r3g321-22.tistory.com/273"
    ]

def get_blog_post_content(url):
    #ë¸”ë¡œê·¸ ë§í¬ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì„œ iframeì—ì„œ í¬ë¡¤ë§
    try:
        driver.get(url)
        time.sleep(1) # Wait briefly for the page to load
        driver.switch_to.frame("mainFrame") # Switch to the iframe containing the blog post content
        
        try:
            #ë³¸ë¬¸ ë‚´ìš© í¬ë¡¤ë§í•˜ê¸°
            content = driver.find_element(By.CSS_SELECTOR,'div.se-main-container').text
        
        except NoSuchElementException:
            # NoSuchElement ì˜¤ë¥˜ì‹œ ì˜ˆì™¸ì²˜ë¦¬(êµ¬ë²„ì „ ë¸”ë¡œê·¸ì— ì ìš©)
            content = driver.find_element(By.CSS_SELECTOR,'div#content-area').text
        
        # Always switch back to main document return content
        driver.switch_to.default_content() 
        return content   

    except Exception as e:
        print(f"Error fetching blog content from {url}: {e}") 
        return None

if __name__ == "__main__":
    for url in links:
        print("\n\n" + "#" * 80)
        print(f"URL: {url}\n")
        raw = get_blog_post_content(url)
        if not raw:
            print("âš ï¸  No content, skipping.")
            continue

        print("ðŸ“„ Raw review text:\n", raw[:500], "â€¦\n")  # print first 500 chars

        try:
            chunks = chunk_review_with_llm(raw)
            print("ðŸ¤– LLMâ€Chunked JSON:")
            print(json.dumps(chunks, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"âŒ Chunking failed: {e}")

    driver.quit()
